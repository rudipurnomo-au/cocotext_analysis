{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This notebook is used to evaluate CTPN text detection, which can be built from here:\n",
    "<br>\n",
    "https://github.com/eragonruan/text-detection-ctpn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To parse coco annot\n",
    "import coco_text\n",
    "# To eval prediction json \n",
    "import coco_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cocotext annotation data, can be downloaded here: https://s3.amazonaws.com/cocotext/COCO_Text.zip\n",
    "GROUND_TRUTH_PATH = r'/home/rudi/Documents/Data/cocotext/COCO_Text.json'\n",
    "# Bounding box annotations generated by CTPN, each record containing: \n",
    "# image id\n",
    "# bounding box [top left x, top left y, width, height]\n",
    "# confidence score\n",
    "EVAL_PATH = r'/home/rudi/Documents/Data/cocotext/cocotext_ctpn_output_score.json'\n",
    "# To compare the bounding box if needed\n",
    "IMAGES_DIR = r'/home/rudi/Documents/Data/cocotext/train2014_text_valid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:01.944672\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Ground truth annotations\n",
    "gt = coco_text.COCO_Text(GROUND_TRUTH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: http://vision.cornell.edu/se3/coco-text/\n",
      "date_created: 2017-03-28\n",
      "version: 1.4\n",
      "description: This is 1.4 version of the 2017 COCO-Text dataset.\n",
      "author: COCO-Text group\n"
     ]
    }
   ],
   "source": [
    "gt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...     \n",
      "DONE (t=15.14s)\n"
     ]
    }
   ],
   "source": [
    "# Evaluated predictions\n",
    "ev = gt.loadRes(EVAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of annotation and image ids with certain filter\n",
    "# Default used for comparison: legible text, machine printed (screw handwriting)\n",
    "valid_image_ids = gt.getImgIds(imgIds=gt.train, \n",
    "                    catIds=[('legibility','legible'),('class','machine printed')])\n",
    "valid_annotation_ids = gt.getAnnIds(imgIds=gt.train, \n",
    "                        catIds=[('legibility','legible'),('class','machine printed')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary of image IDs to their filepath\n",
    "# For visualisation and other misc stuff\n",
    "image_paths = {}\n",
    "for image_name in os.listdir(IMAGES_DIR):\n",
    "    image_id = int(image_name.split('.')[0].split('_')[-1])\n",
    "    image_paths[image_id] = os.path.join(IMAGES_DIR,image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth box of image 81\n",
      "[101, 173, 127.8800759193141, 18.483669688596507]\n",
      "\n",
      "Evaluated box of image 81\n",
      "[90, 175, 147, 19]\n",
      "\n",
      "IOU: 0.6914066132540398\n"
     ]
    }
   ],
   "source": [
    "# Sample annotation comparison\n",
    "image_id = 81\n",
    "\n",
    "gt_ann = gt.loadAnns(gt.getAnnIds(imgIds=image_id))\n",
    "eval_ann = ev.loadAnns(ev.getAnnIds(imgIds=image_id))\n",
    "\n",
    "print('Ground truth box of image '+str(image_id))\n",
    "print(gt_ann[0]['bbox'])\n",
    "print('\\nEvaluated box of image '+str(image_id))\n",
    "print(eval_ann[0]['bbox'])\n",
    "print('\\nIOU: '+str(coco_evaluation.iou_score(gt_ann[0]['bbox'], eval_ann[0]['bbox'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'coco_evaluation' from '/home/rudi/Documents/Notebook/Data_Anal/coco_text/coco_evaluation.py'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload \n",
    "reload(coco_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_results = coco_evaluation.getDetections(gt,ev,\n",
    "                                                  imgIds=valid_image_ids,\n",
    "                                                  annIds=valid_annotation_ids,\n",
    "                                                  detection_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive count : 12656\n",
      "False positive count : 54123\n",
      "False negative count : 53064\n"
     ]
    }
   ],
   "source": [
    "print(f\"True positive count : {len(detection_results['true_positives'])}\")\n",
    "print(f\"False positive count : {len(detection_results['false_positives'])}\")\n",
    "print(f\"False negative count : {len(detection_results['false_negatives'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
